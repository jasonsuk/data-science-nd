{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "So far in this lesson, you have gained some exposure to Singular Value Decomposition.  In this notebook, you will get some hands on practice with this technique.\n",
    "\n",
    "Let's get started by reading in our libraries and setting up the data we will use throughout this notebook.\n",
    "\n",
    "`1.` Run the cell below to create the **user_movie_subset** dataframe.  This will be the dataframe you will use for the first part of this notebook.\n",
    "\n",
    "**Note: Unstacking the matrix here could take ~10 mins to run.** Therefore, after running it, the subsetted data was saved as a csv file `user_movie_subset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import svd_tests as t\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('../movies_clean.csv')\n",
    "reviews = pd.read_csv('../reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']\n",
    "\n",
    "# Create user-by-item matrix\n",
    "# user_items = reviews[['user_id', 'movie_id', 'rating']]\n",
    "# user_by_movie = user_items.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "\n",
    "# user_movie_subset = user_by_movie[[73486, 75314,  68646, 99685]].dropna(axis=0)\n",
    "# print(user_movie_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the subset data for later use\n",
    "# user_movie_subset.to_csv('user_movie_subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now that you have the **user_movie_subset** matrix, use this matrix to correctly match each key to the correct value in the dictionary below.  Use the cells below the dictionary as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>73486</th>\n",
       "      <th>75314</th>\n",
       "      <th>68646</th>\n",
       "      <th>99685</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6571</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11639</th>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         73486  75314  68646  99685\n",
       "user_id                            \n",
       "265       10.0   10.0   10.0   10.0\n",
       "1023      10.0    4.0    9.0   10.0\n",
       "1683       8.0    9.0   10.0    5.0\n",
       "6571       9.0    8.0   10.0   10.0\n",
       "11639     10.0    5.0    9.0    9.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load subset data\n",
    "user_movie_subset = pd.read_csv('user_movie_subset.csv', index_col=0)\n",
    "\n",
    "print(user_movie_subset.shape)\n",
    "user_movie_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  There are 20 users in the dataset, which is given by the number of rows. There are 4 movies in the dataset given by the number of columns.  You can find the movies or users with the highest average ratings by taking the mean of each row or column.  Using the movies table, you can find the movie names associated with each id.  This shows the top rated movie is The Godfather!\n"
     ]
    }
   ],
   "source": [
    "# match each letter to the best statement in the dictionary below - each will be used at most once\n",
    "a = 20\n",
    "b = 68646\n",
    "c = 'The Godfather'\n",
    "d = 'Goodfellas'\n",
    "e = 265\n",
    "f = 30685\n",
    "g = 4\n",
    "\n",
    "sol_1_dict = {\n",
    "    'the number of users in the user_movie_subset': a,\n",
    "    'the number of movies in the user_movie_subset': g,\n",
    "    'the user_id with the highest average ratings given': e,\n",
    "    'the movie_id with the highest average ratings received': b,\n",
    "    'the name of the movie that received the highest average rating': c\n",
    "}\n",
    "\n",
    "\n",
    "#test dictionary here\n",
    "t.test1(sol_1_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 20\n",
      "Number of movies: 4\n",
      "User with the highest average rating: 265\n",
      "Movie with the highest average rating: 68646\n",
      "Movie name with the highest average rating: The Godfather (1972)\n"
     ]
    }
   ],
   "source": [
    "# Workspace\n",
    "\n",
    "# Number of users and movies\n",
    "num_user, num_movies = user_movie_subset.shape\n",
    "print(f'Number of users: {num_user}\\nNumber of movies: {num_movies}')\n",
    "\n",
    "# user with the highest average rating\n",
    "user_top_rating = user_movie_subset.mean(axis=1).nlargest(1).index[0]\n",
    "print(f'User with the highest average rating: {user_top_rating}')\n",
    "\n",
    "# movie with highest average rating\n",
    "movie_top_rating = user_movie_subset.mean(axis=0).nlargest(1).index[0]\n",
    "print(f'Movie with the highest average rating: {movie_top_rating}')\n",
    "\n",
    "# the movie name with the hightest average rating\n",
    "title_top_rating = movies.query(f'movie_id == {movie_top_rating}')['movie'].values[0]   \n",
    "print(f'Movie name with the highest average rating: {title_top_rating}')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a little more context about the matrix we will be performing Singular Value Decomposition on, we're going to do just that.  To get started, let's remind ourselves about the dimensions of each of the matrices we are going to get back.   Essentially, we are going to split the **user_movie_subset** matrix into three matrices:\n",
    "\n",
    "$$ U \\Sigma V^T $$\n",
    "\n",
    "\n",
    "`3.` Given what you learned in the previous parts of this lesson, provide the dimensions for each of the matrices specified above using the dictionary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  We will now put this to use, so you can see how the dot product of these matrices come together to create our user item matrix.  The number of latent features will control the sigma matrix as well, and this will a square matrix that will at most be the minimum of the number of users and number of movies (in our case the minimum is the 4 movies).\n"
     ]
    }
   ],
   "source": [
    "# match each letter in the dictionary below - a letter may appear more than once.\n",
    "a = 'a number that you can choose as the number of latent features to keep'\n",
    "b = 'the number of users'\n",
    "c = 'the number of movies'\n",
    "d = 'the sum of the number of users and movies'\n",
    "e = 'the product of the number of users and movies'\n",
    "\n",
    "sol_2_dict = {\n",
    "    'the number of rows in the U matrix': b, \n",
    "    'the number of columns in the U matrix': a, \n",
    "    'the number of rows in the V transpose matrix': a, \n",
    "    'the number of columns in the V transpose matrix': c\n",
    "}\n",
    "\n",
    "#test dictionary here\n",
    "t.test2(sol_2_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's verify the above dimensions by performing SVD on our user-movie matrix.\n",
    "\n",
    "`4.` Below you can find the code used to perform SVD in numpy.  You can see more about this functionality in the [documentation here](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.svd.html).  What do you notice about the shapes of your matrices?  If you try to take the dot product of the three objects you get back, can you directly do this to get back the user-movie matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4,), (20, 20), (4, 4))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, s, vt = np.linalg.svd(user_movie_subset) # perform svd here on user_movie_subset\n",
    "s.shape, u.shape, vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 75.48556738,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   9.67425201,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   7.42235236,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   5.49969554]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform sigma into diagonal matrix using numpy\n",
    "np.diag(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for our thoughts on the questions posted above\n",
    "# t.question4thoughts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use the thoughts from the above question to create u, s, and vt with four latent features.  When you have all three matrices created correctly, run the test below to show that the dot product of the three matrices creates the original user-movie matrix.  The matrices should have the following dimensions:\n",
    "\n",
    "$$ U_{n x k} $$\n",
    "\n",
    "$$\\Sigma_{k x k} $$\n",
    "\n",
    "$$V^T_{k x m} $$\n",
    "\n",
    "where:\n",
    "\n",
    "1. n is the number of users\n",
    "2. k is the number of latent features to keep (4 for this case)\n",
    "3. m is the number of movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dimensions of u, s, and vt as necessary to use four latent features\n",
    "# update the shape of u and store in u_new\n",
    "u_new = u[:, :len(s)]# change the shape of u here\n",
    "\n",
    "# update the shape of s and store in s_new\n",
    "s_new = np.zeros((len(s), len(s))) # change the shape of s as necessary \n",
    "s_new[:len(s), :len(s)] = np.diag(s)\n",
    "\n",
    "# Because we are using 4 latent features and there are only 4 movies, \n",
    "# vt and vt_new are the same\n",
    "vt_new = vt[:len(s), :]# change the shape of vt as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right! The dimensions of u should be 20 x 4, and both v transpose and sigma should be 4 x 4.  The dot product of the three matrices how equals the original user-movie matrix!\n"
     ]
    }
   ],
   "source": [
    "# Check your matrices against the solution\n",
    "assert u_new.shape == (20, 4), \"Oops!  The shape of the u matrix doesn't look right. It should be 20 by 4.\"\n",
    "assert s_new.shape == (4, 4), \"Oops!  The shape of the sigma matrix doesn't look right.  It should be 4 x 4.\"\n",
    "assert vt_new.shape == (4, 4), \"Oops! The shape of the v transpose matrix doesn't look right.  It should be 4 x 4.\"\n",
    "assert np.allclose(np.dot(np.dot(u_new, s_new), vt_new), user_movie_subset), \"Oops!  Something went wrong with the dot product.  Your result didn't reproduce the original movie_user matrix.\"\n",
    "print(\"That's right! The dimensions of u should be 20 x 4, and both v transpose and sigma should be 4 x 4.  The dot product of the three matrices how equals the original user-movie matrix!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the sigma matrix can actually tell us how much of the original variability in the user-movie matrix is captured by each latent feature.  The total amount of variability to be explained is the sum of the squared diagonal elements.  The amount of variability explained by the first component is the square of the first value in the diagonal.  The amount of variability explained by the second component is the square of the second value in the diagonal.   \n",
    "\n",
    "`6.` Using the above information, can you determine the amount of variability in the original user-movie matrix that can be explained by only using the first two components? Use the cell below for your work, and then test your answer against the solution with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5698.0708826285718, 93.591151890003474, 55.09131448826183, 30.246650993162532]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_list = []\n",
    "\n",
    "for elem in s:\n",
    "    var = np.power(elem,2).sum() \n",
    "    var_list.append(var)\n",
    "    \n",
    "assert sum(var_list) == np.power(s, 2).sum(), \"The sum of individual variability is not equal to total variability\"\n",
    "var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total variance in the original matrix is 5877.0.\n",
      "Ther percentage of variability captured by the first two components is 0.9854793320603328%.\n"
     ]
    }
   ],
   "source": [
    "total_var = np.power(s, 2).sum() # Total variability here\n",
    "var_exp_comp1_and_comp2 = sum(var_list[:2]) # Variability Explained by the first two components here\n",
    "perc_exp = var_exp_comp1_and_comp2 / total_var # Percent of variability explained by the first two components here\n",
    "\n",
    "# Run the below to print your results\n",
    "print(\"The total variance in the original matrix is {}.\".format(total_var))\n",
    "print(\"Ther percentage of variability captured by the first two components is {}%.\".format(perc_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yup!  That all looks good!\n"
     ]
    }
   ],
   "source": [
    "assert np.isclose(np.round(perc_exp, 4), 0.9855), \"Oops!  That doesn't look quite right.  You should have total variability as the sum of all the squared elements in the sigma matrix.  Then just the sum of the squared first two elements is the amount explained by the first two latent features.  Try again.\"\n",
    "print(\"Yup!  That all looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`7.` Similar to the previous question, change the shapes of your u, sigma, and v transpose matrices.  However, this time consider only using the first 2 components to reproduce the user-movie matrix instead of all 4. After you have your matrices set up, check your matrices against the solution by running the tests.  The matrices should have the following dimensions:\n",
    "\n",
    "$$ U_{n x k} $$\n",
    "\n",
    "$$\\Sigma_{k x k} $$\n",
    "\n",
    "$$V^T_{k x m} $$\n",
    "\n",
    "where:\n",
    "\n",
    "1. n is the number of users\n",
    "2. k is the number of latent features to keep (2 for this case)\n",
    "3. m is the number of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dimensions of u, s, and vt as necessary to use four latent features\n",
    "# update the shape of u and store in u_new\n",
    "k = 2\n",
    "\n",
    "u_2 = u[:, :k] # change the shape of u here\n",
    "\n",
    "# update the shape of s and store in s_new\n",
    "s_2 = np.zeros((k,k)) #change the shape of s as necessary \n",
    "s_2[:k, :k] = np.diag(s[:k])\n",
    "\n",
    "# Because we are using 4 latent features and there are only 4 movies, \n",
    "# vt and vt_new are the same\n",
    "vt_2 = vt[:k, :] # change the shape of vt as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right! The dimensions of u should be 20 x 2, sigma should be 2 x 2, and v transpose should be 2 x 4. \n",
      "\n",
      " The question is now that we don't have all of the latent features, how well can we really re-create the original user-movie matrix?\n"
     ]
    }
   ],
   "source": [
    "# Check that your matrices are the correct shapes\n",
    "assert u_2.shape == (20, 2), \"Oops!  The shape of the u matrix doesn't look right. It should be 20 by 2.\"\n",
    "assert s_2.shape == (2, 2), \"Oops!  The shape of the sigma matrix doesn't look right.  It should be 2 x 2.\"\n",
    "assert vt_2.shape == (2, 4), \"Oops! The shape of the v transpose matrix doesn't look right.  It should be 2 x 4.\"\n",
    "print(\"That's right! The dimensions of u should be 20 x 2, sigma should be 2 x 2, and v transpose should be 2 x 4. \\n\\n The question is now that we don't have all of the latent features, how well can we really re-create the original user-movie matrix?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`8.` When using all 4 latent features, we saw that we could exactly reproduce the user-movie matrix.  Now that we only have 2 latent features, we might measure how well we are able to reproduce the original matrix by looking at the sum of squared errors from each rating produced by taking the dot product as compared to the actual rating.  Find the sum of squared error based on only the two latent features, and use the following cell to test against the solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the dot product\n",
    "pred_ratings = u_2.dot(s_2).dot(vt_2)  # store the result of the dot product here\n",
    "\n",
    "# Compute the squared error for each predicted vs. actual rating\n",
    "diff = user_movie_subset - pred_ratings\n",
    "sum_square_errs = np.power(diff, 2).sum().sum() # compute the sum of squared differences from each prediction to each actual value here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That looks right!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "# Check against the solution\n",
    "assert np.round(sum_square_errs, 2) == 85.34, \"Oops!  That doesn't look quite right.  You should return a single number for the whole matrix.\"\n",
    "print(\"That looks right!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you may be thinking . . . why would we want to choose a k that doesn't just give us back the full user-movie matrix with all the original ratings.  This is a good question.  One reason might be for computational reasons - sure, you may want to reduce the dimensionality of the data you are keeping, but really this isn't the main reason we would want to perform reduce k to lesser than the minimum of the number of movies or users.\n",
    "\n",
    "Let's take a step back for a second.  In this example we just went through, your matrix was very clean.  That is, for every user-movie combination, we had a rating.  **There were no missing values.** But what we know from the previous lesson is that the user-movie matrix is full of missing values.  \n",
    "\n",
    "A matrix similar to the one we just performed SVD on:\n",
    "\n",
    "<img src=\"images/nice_ex.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "The real world:\n",
    "\n",
    "<img src=\"images/real_ex.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "\n",
    "Therefore, if we keep all k latent features it is likely that latent features with smaller values in the sigma matrix will explain variability that is probably due to noise and not signal. Furthermore, if we use these \"noisey\" latent features to assist in re-constructing the original user-movie matrix it will potentially (and likely) lead to worse ratings than if we only have latent features associated with signal.   \n",
    "\n",
    "`9.` Let's try introducing just a little of the real world into this example by performing SVD on a matrix with missing values.  Below I have added a new user to our matrix who hasn't rated all four of our movies.  Try performing SVD on the new matrix.  What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-ce9c19dd238d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Try svd with this new matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_movie_subset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Compute SVD on the new matrix with the single nan value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_svd_nonconvergence\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_svd_nonconvergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVD did not converge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: SVD did not converge"
     ]
    }
   ],
   "source": [
    "# This line adds one nan value as the very first entry in our matrix\n",
    "user_movie_subset.iloc[0, 0] = np.nan # no changes to this line\n",
    "\n",
    "# Try svd with this new matrix\n",
    "u, s, vt = np.linalg.svd(user_movie_subset) # Compute SVD on the new matrix with the single nan value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your response here.** \n",
    "\n",
    "`LinAlgError` is produced and tells SVD did not converge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario\n",
    "\n",
    "Let's say that you're working for a fictional productivity software company that is looking for ways to increase the number of people who pay for their software. The way that the software is currently set up, users can download and use the software free of charge, for a 7-day trial. After the end of the trial, users are required to pay for a license to continue using the software.\n",
    "\n",
    "One idea that the company wants to try is to change the layout of the homepage to emphasize more prominently and higher up on the page that there is a 7-day trial available for the company's software. The current fear is that some potential users are missing out on using the software because of a lack of awareness of the trial period. If more people download the software and use it in the trial period, the hope is that this entices more people to make a purchase after seeing what the software can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to see more!</summary>\n",
    "  \n",
    "    In this case study, you'll go through steps for planning out an experiment to test the new homepage. You will start by constructing a user funnel and deciding on metrics to track. You'll also perform experiment sizing to see how long it should be run. Afterwards, you'll be given some data collected for the experiment, perform statistical tests to analyze the results, and come to conclusions regarding how effective the new homepage changes were for bringing in more users.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Funnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of the study\n",
    "Redesign the homepage to increase the number of people who download the software and eventually the number of people who purchase a license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The company's website has five main sections:\n",
    "\n",
    "1. the homepage;\n",
    "2. a section with additional information, gallery, and examples;\n",
    "3. a page for users to download the software;\n",
    "4. a page for users to purchase a license; and\n",
    "5. a support sub-site with documentation and FAQs for the software.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click to see more!</summary>\n",
    "    \n",
    "    For the software itself, the website requires that users create an account in order to download the software program. The program is usable freely for seven days after download. When the trial period is hit, the program will bring up a dialog box that takes the user to the license page. After purchasing a license, the user will receive a unique code associated with their site account. This code can then be used with the program to register it with that user, and the program can be used thereafter without issue.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Expected flow\n",
    "What steps do you expect typical visitors to take from their initial visit to the webpage through purchasing a license for continued use of the program? Are there any 'typical' steps that certain visitors might not take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Example answer here</summary>\n",
    "  \n",
    "A straightforward flow might include the following steps:\n",
    "\n",
    "- Visit homepage\n",
    "- Visit download page\n",
    "- Sign up for an account\n",
    "- Download software\n",
    "- After 7-day trial, software takes user to license-purchase page\n",
    "- Purchase license\n",
    "\n",
    "    Note that it is possible for the visitor to drop from the flow after each step, forming a funnel. There might be additional steps that a user might take between visiting the homepage and visiting the download page that aren't accounted for in the above flow. For example, someone might want to check out the additional informational pages before visiting the download page, or even visit the license purchase page to check the license price before even deciding to download. Considering the amount of browsing that a visitor could perform on the page, it might be simplest just to track whether or not a user gets to the download page at some point, without worrying about the many paths that they could have taken to get there.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Atypical events\n",
    "Consider the webpage as a whole. What kinds of events might occur outside of the expected flow for the experiment that might interfere with measuring the effects of our manipulation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Example answer here</summary>\n",
    "   \n",
    "  There are a few events in the expected flow that might not correspond with the visitors we want to target. For example, there might be users on the homepage who aren't new users. Users who already have a license might just be visiting the homepage as a way to access the support sub-site. A user who wants to buy a license might also come in to the license page through the homepage, rather than directly from the software.\n",
    "\n",
    "When it comes to license purchasing, it's possible that users don't come back after exactly seven days. Some users might come back early and make their purchase during their trial period. Alternatively, a user might end up taking more than seven days to decide to make their purchase, coming back days after the end of the trial. Anticipating scenarios like this can be useful for planning the design, and coming up with metrics that come as close as possible to measuring desired effects.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding on metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our user funnel, we should consider two things: where and how we should split users into experiment groups, and what metrics we will use to track the success or failure of the experimental manipulation. The choice of unit of diversion (the point at which we divide observations into groups) may affect what metrics we can use, and whether the metrics we record should be considered invariant or evaluation metrics. To start, decide on a unit of diversion and brainstorm some ideas for metrics to capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click to expand</summary>\n",
    "\n",
    "To be clear, the overall plan is to test the effect of the new homepage using a true experiment; in particular, we'll be using an A/B testing framework. This means that prospective users should be split into two groups. The control, or 'A' group, will see the old homepage, while the experimental, or 'B' group, will see the new homepage that emphasizes the 7-day trial. </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "By which means should we divide visitors into our experimental and control groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Example answer here</summary>\n",
    "\n",
    "A cookie-based diversion seems best in this case for dividing visitors into experimental groups since we can split visitors on their initial visit and it's fairly reliable for tracking. We can assign a cookie to each visitor upon their first page hit, which allows them to be separated into the control and experimental groups. Cookies also allow tracking of each visitor hitting each page, recording whether or not they eventually hit the download page and then whether or not they actually register an account and perform the download. \n",
    "    \n",
    "The usual cookie-based diversion issues apply: we can get some inconsistency in counts if users enter the site via incognito window, different browsers, or cookies that expire or get deleted before they make a download. This kind of assignment 'dilution' could dampen the true effect of our experimental manipulation. As a simplification, however, we'll assume that this kind of assignment dilution will be small, and ignore its potential effects.\n",
    "    \n",
    "Other methods: event-based diversion (i.e. pageview), account-based divsersion (i.e. User ID).\n",
    "    \n",
    "An event-based diversion (like a pageview) can provide many observations to draw conclusions from, but doesn't quite hit the mark for this case. If the condition changes on each pageview, then a visitor might get a different experience on each homepage visit. Event-based diversion is much better when the changes aren't as easily visible to users, to avoid disruption of experience. In addition, pageview-based diversion would let us know how many times the download page was accessed from each condition, but can't go any further in tracking how many actual downloads were generated from each condition.\n",
    "    \n",
    "Diverting based on account or user ID can be stable, but it's not the right choice in this case. Since visitors only register after getting to the download page, this is too late to introduce the new homepage to people who should be assigned to the experimental condition.    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Write down some potential metrics or ways of objectively measuring things related to evaluating the success of the experiment. You don't need to decide on invariant or evaluation metrics here: that'll be discussed in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Example answer here</summary>\n",
    "    \n",
    "In terms of metrics, we might want to keep track of the number of cookies that are recorded in different parts of the website. In particular, the number of cookies on the homepage, download page, and account registration page (in order to actually make the download) could prove useful. We can track the number of licenses purchased through the user accounts, each of which can be linked back to a particular condition. Though it hasn't been specified, it's also possible that the software includes usage statistics that we could track.\n",
    "\n",
    "The above metrics are all based on absolute counts. We could instead perform our analysis on ratios of those counts. For example, we could be interested in the proportion of downloads out of all homepage visits. License purchases could be stated as a ratio against the number of registered users (downloads) or the original number of cookies.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Below, you will decide for each of the proposed metrics whether or not you would want to use them as an invariant metric or an evaluation metric.\n",
    "\n",
    "An invariant metric is an objective measure that you should expect will not vary between conditions and that indicate equivalence between groups. Evaluation metrics, on the other hand, represent measures where you expect there will be differences between groups, and whose differences should say something meaningful about your experimental manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Number of cookies @ homepage\n",
    "2. Number of cookies @ download page\n",
    "3. Number of user ids / downloads\n",
    "4. Number of license purchases\n",
    "5. Mean software usage time during trial\n",
    "6. Ratio: # downloads / # cookies\n",
    "7. Ratio: # licenses / # cookies\n",
    "8. Ratio: # licenses / # user IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Example answer here</summary>\n",
    "    \n",
    "`Invariant:` 1, each visitor should have an equal chance of seeing each homepage. Since visitors come in without any additional information (e.g. account info) and the change effected by the experimental manipulation comes in right at the start, there aren't other invariant metrics we should worry about.\n",
    "    \n",
    "`Evaluation:` 4, 6, 7\n",
    "    \n",
    "Count-based metrics at other parts of the process seem like natural choices: the number of times the software was downloaded and the number of licenses purchased are exactly what we want to change with the new homepage. The issue is that even though we expect the number of cookies assigned to each group to be about the same, it's much more likely than not they they won't be exactly the same. Instead, we should prefer using the download rate (# downloads / # cookies) and purchase rate (# licenses / # cookies) relative to the number of cookies as evaluation metrics. Using these ratios allows us to account for slight imbalances between groups.    \n",
    "    \n",
    "As for the other proposed metrics, the ratio between the number of licenses and number of downloads is potentially interesting, but not as direct as the other two ratios discussed above. It's possible that the manipulation increases both the number of downloads and number of licenses, but increases the former to a much higher rate. In this case, the licenses-to-downloads ratio might be worse off for the new homepage compared to the old, even though the new homepage has our desired effects. There's no such inconsistency issue with the ratios that use the number of cookies in the denominator.\n",
    "    \n",
    "Product usage statistics like the average time the software was used in the trial period are potentially interesting features, but aren't directly related to our experiment. We might not have a strong feeling about what kind of effect the homepage will have on people that actually download the software. Stated differently, product usage isn't a direct target of the homepage manipulation. Certainly, these statistics might help us dig deeper into the reasons for observed effects after an experiment is complete. They might even point toward future changes and experiments to conduct. But in terms of experiment success, product usage shouldn't be considered an invariant or evaluation metric.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment sizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have our main metrics selected: number of cookies as an invariant metric, and the download rate and license purchase rate (relative to number of cookies) as evaluation metrics, we should take a look at the feasibility of the experiment in terms of the amount of time it will take to run. We can use historical data as a baseline to see what it might take to detect our desired levels of change.\n",
    "\n",
    "Recent history shows that there are about 3250 unique visitors per day, with slightly more visitors on Friday through Monday, than the rest of the week. There are about 520 software downloads per day (a .16 rate) and about 65 licenses purchased each day (a .02 rate). In an ideal case, both the download rate and license purchase rate should increase with the new homepage; a statistically significant negative change should be a sign to not deploy the homepage change. However, if only one of our metrics shows a statistically significant positive change we should be happy enough to deploy the new homepage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Consider that we want to preserve a maximum 5% Type I error rate for falsely deploying the homepage without any actual effect. Should we apply the Bonferroni correction in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Example answer here</summary>\n",
    "    \n",
    "Since we're willing to deploy the homepage with an increase in only one of our two metrics (download rate, license purchase rate), we need to apply the Bonferroni correction to avoid making too many false positives due to multiple testing.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qustion\n",
    "Let's say that we want to detect an increase of 50 downloads per day (up to 570 per day, or a .175 rate). How many days of data would we need to collect in order to get enough visitors to detect this new rate at an overall 5% Type I error rate and at 80% power?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hint here</summary>\n",
    "    \n",
    "Formula: n = ((z_null*sd_null - z_alt*sd_alt) / p_diff) ** 2\n",
    "        \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014999999999999986\n"
     ]
    }
   ],
   "source": [
    "num_metrics = 2\n",
    "alpha = 0.05\n",
    "beta = 0.8\n",
    "\n",
    "bonf_alpha = alpha / num_metrics\n",
    "\n",
    "p_null = 0.16\n",
    "p_alt = 0.175\n",
    "p_diff = p_alt - p_null\n",
    "print(p_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_null = stats.norm.ppf(1-bonf_alpha)\n",
    "z_alt = stats.norm.ppf(1-beta)\n",
    "sd_null = np.sqrt(p_null*(1-p_null) + p_null*(1-p_null))\n",
    "sd_alt =np.sqrt(p_null*(1-p_null) + p_alt*(1-p_alt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.959963984540054,\n",
       " -0.8416212335729143,\n",
       " 0.5184592558726288,\n",
       " 0.5279914772039412)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_null, z_alt, sd_null, sd_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9480.661238251349\n"
     ]
    }
   ],
   "source": [
    "n = ((z_null*sd_null - z_alt*sd_alt) / p_diff) ** 2\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So n=9481 for each group. The number of visitors is 3250 per day, which needs to be divided by 2 (control/experiment). Therefore, we need 6 days approx (9481/1625)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What if we wanted to detect an increase of 10 license purchases per day (up to 75 per day, or a .023 rate). How many days of data would we need to collect in order to get enough visitors to detect this new rate at an overall 5% Type I error rate and at 80% power?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 \n",
    "beta = 0.8\n",
    "num_metrics = 2\n",
    "bonf_alpha = alpha / num_metrics\n",
    "\n",
    "p_null = 0.02\n",
    "p_alt = 0.023\n",
    "p_diff = p_alt - p_null\n",
    "\n",
    "z_null = stats.norm.ppf(1-bonf_alpha)\n",
    "z_alt = stats.norm.ppf(1-beta) \n",
    "sd_null = np.sqrt(p_null*(1-p_null) + p_null*(1-p_null))\n",
    "sd_alt = np.sqrt(p_null*(1-p_null) + p_alt*(1-p_alt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.959963984540054,\n",
       " -0.8416212335729143,\n",
       " 0.1979898987322333,\n",
       " 0.2051121644369246)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_null, z_alt, sd_null, sd_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34929.096074778325\n"
     ]
    }
   ],
   "source": [
    "n = ((z_null*sd_null - z_alt*sd_alt) / p_diff) ** 2\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the daily visit of 3250, we need 1625 experiment size per day for each group. This requires the collection period of 21.49 days (34929 / 1625)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answers for the 2 questions</summary>\n",
    "    \n",
    "For an overall 5% Type I error rate with Bonferroni correction and 80% power, we should require 6 days (rounded up from 5.55) to reliably detect a 50 download increase per day and 21 days (rounded up from 20.44) to detect an increase of 10 license purchases per day. 21 days is actually a convenient number since the three-week timespan helps to account for weekly cycles. In addition, the 21-day data collection period is a short enough timeframe that running the experiment is a reasonable proposition. If the required experiment length were a few weeks longer, then we might have needed to forego measuring the license purchasing rate as a critical metric.  \n",
    "    \n",
    "One thing that isn't accounted for in the base experiment length calculations is that there is going to be a delay between when users download the software and when they actually purchase a license. That is, when we start the experiment, there could be about seven days before a user account associated with a cookie actually comes back to make their purchase. Any purchases observed within the first week might not be attributable to either experimental condition. As a way of accounting for this, we'll run the experiment for about one week longer to allow those users who come in during the third week a chance to come back and be counted in the license purchases tally.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for Validity, Bias, and Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Expand to explore more!</summary>\n",
    "        \n",
    "Before getting to the data and its analysis, let's review a few of the conceptual points that go into creation of an experiment: validity, bias, and ethics.\n",
    "\n",
    "We probably don't have too much to worry about in terms of validity. For conceptual validity, the evaluation metrics are directly aligned with the experimental goals, no abstraction needed. Internal validity is maintained by performing an experiment with properly-handled randomization and controls. We don't really need to answer to external validity since we're drawing from the full site population and there's no other population we're looking to generalize to.\n",
    "\n",
    "As for biases, we might think of novelty bias as being a potential issue. However, we don't expect users to come back to the homepage regularly. Downloading and license purchasing are actions we expect to only occur once per user, so there's no real 'return rate' to worry about. One possibility, however, is that if more people download the software under the new homepage, the expanded user base is qualitatively different from the people who came to the page under the original homepage. This might cause more homepage hits from people looking for the support pages on the site, causing the number of unique cookies under each condition to differ. If we do see something wrong or out of place in the invariant metric (number of cookies), then this might be an area to explore in further investigations.\n",
    "\n",
    "Finally, for ethical issues, the changes to the homepage should be benign and present no risk to users. Our experiment objectives are also clearly stated. Considering the low risks of the experiment, informed consent is at worst a minor concern; a standard popup to let visitors know that cookies are used to track user experience on the site will likely suffice. The largest ethics principle we should be concerned about is data sensitivity. We shouldn't get any sensitive data out of the cookie assignment and collection, though some information will be collected from the user when they go to download the software. No sensitive data is required for the metrics we've laid out, so what we should do is just aggregate daily visits, downloads, and purchase counts without looking at any individual outcomes.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Data\n",
    "Let's assume that the experiment was given the green light to go ahead, and data was collected for 29 days. As a reminder of the discussion on experiment sizing, it was found that a three-week period was needed to collect enough visitors to achieve our desired power level. Eight additional days of collection were added to allow visitors in the last week to complete their trials and come back to make a purchase – if you look at the data linked in the next paragraph, you will see that it takes about eight days before the license purchases reaches its steady level.\n",
    "\n",
    "The data file reports the daily counts for the number of unique cookies, number of downloads, and number of license purchases attributed to each group: the experimental group with the new homepage, or the control group with the old homepage. The number of license purchases only includes purchases by users who joined after the start of the experiment, so there will be some time before the counts reach their steady state. As noted earlier, we'll assume that the potentially muddying effects of visits across multiple days, established user visits, and 'lost' cookie tracking will be ignorable, at least unless we find reason to doubt our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Control Cookies</th>\n",
       "      <th>Control Downloads</th>\n",
       "      <th>Control Licenses</th>\n",
       "      <th>Experiment Cookies</th>\n",
       "      <th>Experiment Downloads</th>\n",
       "      <th>Experiment Licenses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1764</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>1850</td>\n",
       "      <td>339</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1541</td>\n",
       "      <td>234</td>\n",
       "      <td>2</td>\n",
       "      <td>1590</td>\n",
       "      <td>281</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1457</td>\n",
       "      <td>240</td>\n",
       "      <td>1</td>\n",
       "      <td>1515</td>\n",
       "      <td>274</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1587</td>\n",
       "      <td>224</td>\n",
       "      <td>1</td>\n",
       "      <td>1541</td>\n",
       "      <td>284</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1606</td>\n",
       "      <td>253</td>\n",
       "      <td>2</td>\n",
       "      <td>1643</td>\n",
       "      <td>292</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Control Cookies  Control Downloads  Control Licenses  \\\n",
       "0    1             1764                246                 1   \n",
       "1    2             1541                234                 2   \n",
       "2    3             1457                240                 1   \n",
       "3    4             1587                224                 1   \n",
       "4    5             1606                253                 2   \n",
       "\n",
       "   Experiment Cookies  Experiment Downloads  Experiment Licenses  \n",
       "0                1850                   339                    3  \n",
       "1                1590                   281                    2  \n",
       "2                1515                   274                    1  \n",
       "3                1541                   284                    2  \n",
       "4                1643                   292                    3  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = 'data/homepage-experiment-data.csv'\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariant Metric\n",
    "First, we should check our invariant metric, the number of cookies assigned to each group. If there is a statistically significant difference detected, then we shouldn't move on to the evaluation metrics right away. We'd need to first dig deeper to see if there was an issue with the group-assignment procedure, or if there is something about the manipulation that affected the number of cookies observed, before we feel secure about analyzing and interpreting the evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What is the p-value for the test on the number of cookies assigned to each group? Answers are at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we want to see how difference the number of observations for each group is when compared to null (equal count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ctrl = data['Control Cookies'].sum()\n",
    "n_expr = data['Experiment Cookies'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define null\n",
    "n_obs = n_ctrl + n_expr\n",
    "p = 0.5\n",
    "\n",
    "null_mean = n_obs*p\n",
    "null_sd = np.sqrt(n_obs*p*(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z score: -1.6095646049678511\n",
      "P value: 0.10749294050130412\n"
     ]
    }
   ],
   "source": [
    "# Analytic approach\n",
    "z = (n_ctrl+0.5 - null_mean) / null_sd\n",
    "p_val = 2 * stats.norm.cdf(z)\n",
    "\n",
    "print(f'Z score: {z}\\nP value: {p_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_value: 0.105835\n"
     ]
    }
   ],
   "source": [
    "# Simulation approach\n",
    "n_sim = 200_000\n",
    "samples = np.random.binomial(n_obs, p, n_sim)\n",
    "\n",
    "p_val = np.logical_or(samples <= n_ctrl, \n",
    "                      samples >= (n_obs-n_ctrl)).mean()\n",
    "\n",
    "print(f'P_value: {p_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the conventional alpha of 0.05 (two-tailed), the computed p value suggests not enough statistical significance to reject the null where number of observations for two groups are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "Assuming that the invariant metric passed inspection, we can move on to the evaluation metrics: download rate and license purchasing rate. For a refresher, the download rate is the total number of downloads divided by the number of cookies, and the license purchasing rate the number of licenses divided by the number of cookies.\n",
    "\n",
    "One tricky point to consider is that there is a seven or eight day delay between when most people download the software and when they make a purchase. There's no direct way of attributing cookies all the way through license purchases due to the daily aggregation of results, so the best we can do is to make a justified argument for handling the data. To answer the question below about the license purchasing rate, you should only take the cookies observed through day 21 as the denominator of the ratio as being responsible for all of the license purchases observed. (A more informed model of license purchasing could come up with a different handling of the data, such as including part of the day 22 cookies in the denominator.) (Note that we don't need to perform this kind of correction for the download rate, since the link between homepage visits and downloads is much closer.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What is the p-value for the test on the download rate between groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control group\n",
      "    Number of observation: 46851\n",
      "    Proportion of downloads: 0.1612\n",
      "\n",
      "Experiment group\n",
      "    Number of observation: 47346\n",
      "    Proportion of downloads: 0.1805\n"
     ]
    }
   ],
   "source": [
    "# Number of observations\n",
    "n_ctrl = data['Control Cookies'].sum()\n",
    "n_expr = data['Experiment Cookies'].sum()\n",
    "\n",
    "# Number of downloads\n",
    "n_download_ctrl = data['Control Downloads'].sum() \n",
    "n_download_expr = data['Experiment Downloads'].sum()\n",
    "\n",
    "# Proportion of downloads\n",
    "p_download_ctrl = n_download_ctrl / n_ctrl\n",
    "p_download_expr = n_download_expr / n_expr\n",
    "\n",
    "print('Control group')\n",
    "print(f'    Number of observation: {n_ctrl}')\n",
    "print(f'    Proportion of downloads: {p_download_ctrl:.4f}')\n",
    "print()\n",
    "print('Experiment group')\n",
    "print(f'    Number of observation: {n_expr}')\n",
    "print(f'    Proportion of downloads: {p_download_expr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pooled sample proportion and standard error for null\n",
    "\n",
    "# Pooled sample proportion\n",
    "# p_total = (n_ctrl * p_download_ctrl + n_expr * p_download_expr) / (n_ctrl + n_expr)\n",
    "p_download = (n_download_ctrl + n_download_expr) / (n_ctrl + n_expr)\n",
    "\n",
    "# Pooled standard error\n",
    "p_se = np.sqrt(p_download * (1-p_download) * (1/n_ctrl + 1/n_expr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z score: 7.870833726066236\n",
      "P value: 1.7763568394002505e-15\n"
     ]
    }
   ],
   "source": [
    "# Compute z score \n",
    "z = (p_download_expr-p_download_ctrl) / p_se\n",
    "print(f'Z score: {z}')\n",
    "\n",
    "# Compute p-value\n",
    "pval = (1-stats.norm.cdf(z))\n",
    "print(f'P value: {pval}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference at the link [here](https://stattrek.com/hypothesis-test/difference-in-proportions.aspx)\n",
    "\n",
    "<details>\n",
    "    <summary>Analyze Sample Data</summary>\n",
    "      \n",
    "Using sample data, complete the following computations to find the test statistic and its associated P-Value.\n",
    "\n",
    "`Pooled sample proportion` Since the null hypothesis states that P1=P2, we use a pooled sample proportion (p) to compute the standard error of the sampling distribution.\n",
    "$p$ = $(p1 * n1 + p2 * n2) / (n1 + n2)$\n",
    "\n",
    "where p1 is the sample proportion from population 1, p2 is the sample proportion from population 2, n1 is the size of sample 1, and n2 is the size of sample 2.\n",
    "\n",
    "`Standard error` Compute the standard error (SE) of the sampling distribution difference between two proportions.\n",
    "$SE$ = $\\sqrt{(p * ( 1 - p ) * ( \\frac{1}{n_1} + \\frac{1}{n_2} )}$\n",
    "\n",
    "where p is the pooled sample proportion, n1 is the size of sample 1, and n2 is the size of sample 2.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What is the p-value for the test on the license purchasing rate between groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control group\n",
      "    Number of observations: 33758\n",
      "    Proportion of license purchases: 0.0210\n",
      "\n",
      "Experiment group\n",
      "    Number of downloads: 34338\n",
      "    Proportion of license purchases: 0.0213\n"
     ]
    }
   ],
   "source": [
    "# Number of downloads (< Day 22)\n",
    "n_21day_ctrl = data.query('Day < 22')['Control Cookies'].sum()\n",
    "n_21day_expr = data.query('Day < 22')['Experiment Cookies'].sum()\n",
    "\n",
    "# Number of licenses\n",
    "n_license_ctrl = data['Control Licenses'].sum()\n",
    "n_license_expr = data['Experiment Licenses'].sum()\n",
    "\n",
    "# Proportion of licenses\n",
    "p_license_ctrl = n_license_ctrl / n_21day_ctrl\n",
    "p_license_expr = n_license_expr / n_21day_expr\n",
    "\n",
    "print('Control group')\n",
    "print(f'    Number of observations: {n_21day_ctrl}')\n",
    "print(f'    Proportion of license purchases: {p_license_ctrl:.4f}')\n",
    "print()\n",
    "print('Experiment group')\n",
    "print(f'    Number of downloads: {n_21day_expr}')\n",
    "print(f'    Proportion of license purchases: {p_license_expr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct null\n",
    "p_license = (n_license_ctrl + n_license_expr) / (n_21day_ctrl + n_21day_expr)\n",
    "p_se_license = np.sqrt(p_license\n",
    "                       * (1-p_license) \n",
    "                       * (1/n_21day_ctrl + 1/n_21day_expr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z score: 0.2586750111658684\n",
      "P value: 0.3979430008399871\n"
     ]
    }
   ],
   "source": [
    "# Compute z-score\n",
    "z = (p_license_expr - p_license_ctrl) / p_se_license\n",
    "print(f'Z score: {z}')\n",
    "\n",
    "# Compute p-value\n",
    "pval = (1-stats.norm.cdf(z)) \n",
    "print(f'P value: {pval}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Example answers for all (3) questions here</summary>\n",
    "\n",
    "For the test of the invariant metric, number of cookies, there were a larger number of cookies recorded in the experiment group, 47 346 vs. 46 851. This ends up generating a p-value of 0.107 (z = -1.61), which is within a reasonable range under the null hypothesis. Since we lack sufficient reason to reject the null, we can continue on to evaluating the evaluation metrics. (Note that this doesn't mean that there wasn't something actually different about the cookie counts between groups, only that we couldn't detect it if such a difference existed.)\n",
    "\n",
    "For the first evaluation metric, download rate, there was an extremely convincing effect. An absolute increase from 0.1612 to 0.1805 results in a z-score of 7.87, well beyond any standard significance bound. However, the second evaluation metric, license purchasing rate, only shows a small increase from 0.0210 to 0.0213 (following the assumption that only the first 21 days of cookies account for all purchases). This results in a p-value of 0.398 (z = 0.26).    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the results\n",
    "\n",
    "Invariant metric shows the p value of 0.11, which does not prove statistic significance to reject the null where we assume no difference in control and experiment group. \n",
    "\n",
    "The evaluation metrics show that\n",
    "- download rate is significantly different bewteen experiment and control group (one-tail, higher for experiment)\n",
    "- license rate however does not show much of difference as suggested by p value of 0.398."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Despite the fact that statistical significance wasn't obtained for the number of licenses purchased, the new homepage appeared to have a strong effect on the number of downloads made. Based on our goals, this seems enough to suggest replacing the old homepage with the new homepage. Establishing whether there was a significant increase in the number of license purchases, either through the rate or the increase in the number of homepage visits, will need to wait for further experiments or data collection.\n",
    "\n",
    "One inference we might like to make is that the new homepage attracted new users who would not normally try out the program, but that these new users didn't convert to purchases at the same rate as the existing user base. This is a nice story to tell, but we can't actually say that with the data as given. In order to make this inference, we would need more detailed information about individual visitors that isn't available. However, if the software did have the capability of reporting usage statistics, that might be a way of seeing if certain profiles are more likely to purchase a license. This might then open additional ideas for improving revenue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
